{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function Calling with Gemini — Run in Google Colab\n",
        "\n",
        "➡ **Open this notebook in Google Colab.**\n",
        "\n",
        "### Watch the series on YouTube\n",
        "- Part 1: https://youtu.be/sD6czjVekxk\n",
        "- Part 2: https://youtu.be/IWpq4KubUCc\n",
        "- Part 1: https://youtu.be/ePQ_vFhHZfs\n",
        "\n",
        "**Setup tip:** In Colab, add a secret `GEMINI_API_KEY`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fT9WB3MJWfMl"
      },
      "source": [
        "# Function Calling\n",
        "**Function calling** (also known as tool calling) provides a powerful and flexible way to interface with external systems and access data outside their training data. Function calling has 3 primary use cases:\n",
        "<br>\n",
        "\n",
        "* **Augment Knowledge**: Access information from external sources like databases, APIs, and knowledge bases.\n",
        "* **Extend Capabilities**: Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.\n",
        "* **Take Actions**: Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFIgGSEEXxMp"
      },
      "source": [
        "## Terminology\n",
        "\n",
        "### Tools - functionality we give the model\n",
        "\n",
        "\n",
        "*   What is it:\n",
        "    * It’s a specific **capability** you tell the model it can use—think of it as a button the model is allowed to press.\n",
        "    * As the model generates a response to your prompt, it may **decide it needs that capability** (data or an action) to follow your instructions accurately.\n",
        "\n",
        "*   When does model use it:\n",
        "    * Sometimes the prompt requires **fresh data** (today’s weather, an account balance) or a **real action** (issuing a refund).\n",
        "    * The model recognizes that its own knowledge isn’t enough and **chooses a tool you’ve exposed** to complete the task.\n",
        "\n",
        "* Examples of tools you might expose\n",
        "  * Get today’s weather for a specific location.\n",
        "  * Access account details for a given user ID.\n",
        "  * Issue a refund for a lost order.\n",
        "\n",
        "In practice, it can be anything you want the model to know or do while responding—APIs, databases, internal services, automations. <br><br>\n",
        "\n",
        "### Tool calls - requests from the model to use tools\n",
        "\n",
        "* What it is: **A special kind of model response** where, after reading your prompt, the model decides it needs one of your tools to follow the instructions.\n",
        "\n",
        "* When it happens: During generation, the model inspects the prompt and concludes that **external data or an action is required** (something it can’t reliably produce from training data alone).\n",
        "\n",
        "**Plain-English example**\n",
        "```\n",
        "Prompt: “What is the weather in Paris?”\n",
        "\n",
        "Model’s response (conceptually): “I should call get_weather with { location: 'Paris' }.”\n",
        "```\n",
        "<br><br>\n",
        "### Tool call outputs - output we generate for the model\n",
        "* What the model returns: Only **structured data** with the **tool name** and **parameters** (e.g., get_current_weather(location=\"Boston\")).\n",
        "\n",
        "* What we must do: **Our application executes the tool**, produces an **output** (JSON or text), and then **feeds that output back to the model** so it can finish the reply.\n",
        "\n",
        "* Why this matters: This bridges the model to our **systems/APIs**—the model doesn’t call external services by itself.\n",
        "\n",
        "**Weather example**\n",
        "\n",
        "Prompt: ```“What’s the weather in Paris?”```\n",
        "\n",
        "\n",
        "* Model’s tool call (proposal): ```get_weather({ \"location\": \"Paris\" })``` (model does not execute it)\n",
        "\n",
        "* We execute: Call our ```get_weather``` service and get:\n",
        "```\n",
        "{ \"temperature\": \"25\", \"unit\": \"C\" }\n",
        "```\n",
        "\n",
        "\n",
        "* We return to the model: tool definition + original prompt + model’s tool call + our tool output.\n",
        "\n",
        "* Model’s final answer: ```“The weather in Paris today is 25 C.”```\n",
        "<br><br>\n",
        "\n",
        "### Functions vs. tools — what’s the difference?\n",
        "\n",
        "* **Tool (umbrella term)**:\n",
        "Any capability you tell the model it can use. The model may request a tool; your app (or platform) actually runs it.\n",
        "\n",
        "  * **Function** (a schema-defined tool):\n",
        "  A tool described with a JSON schema (name + typed parameters). The model returns function name + args; your code executes and sends the result back. Great for predictable, validated inputs.\n",
        "\n",
        "  * **Custom tools (free-text I/O)**:\n",
        "  Tools that take and/or return plain text instead of strict JSON (Python code as a response). Useful when the interface is flexible or hard to formalize. You still run them on your side.\n",
        "\n",
        "  * **Built-in platform tools**:\n",
        "  Tools provided by the platform (e.g., web search, code execution, MCP server access). You enable/configure them; the model can call them without you writing the tool backend.\n",
        "\n",
        "\n",
        "* **Execution model (always)**:\n",
        "The model doesn’t execute tools. It proposes a call; you (or the platform) run it, then you return the output so the model can finish the answer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynZE0rjGHJC4"
      },
      "source": [
        "## Function Calling Flow\n",
        "![Diagram](../images/function-calling.png)\n",
        "\n",
        "1. **User → Application: Prompt arrives**\n",
        "\n",
        "* The user asks something.\n",
        "\n",
        "* Example: ```“What’s the weather in Paris right now?” ```\n",
        "<br>\n",
        "\n",
        "2. **Application → Model: Send prompt + tool list**\n",
        "\n",
        "* Your app forwards the prompt and the available tools (function declarations).\n",
        "\n",
        "* Example tools: ```get_weather(location), convert_currency(amount, from, to)```\n",
        "<br>\n",
        "\n",
        "3. **Model → Application: Proposes a tool call**\n",
        "\n",
        "* The model does not run code. It returns name + JSON args for a tool if needed.\n",
        "\n",
        "* Example function call: ``` { \"name\": \"get_weather\", \"args\": { \"location\": \"Paris, FR\" } } ```\n",
        "<br>\n",
        "\n",
        "4. **Application → API: You execute the tool**\n",
        "\n",
        "* Your code validates args and calls your API/service.\n",
        "\n",
        "* Example request: ```GET /weather?location=Paris%2C%20FR```\n",
        "\n",
        "* Example API response: ``` { \"temperature\": 25, \"unit\": \"C\", \"condition\": \"Partly cloudy\" }```\n",
        "<br>\n",
        "\n",
        "5. **Application → Model: Return the tool result**\n",
        "\n",
        "* Send back the function response (and the original tool call) so the model can use real data.\n",
        "\n",
        "* Example function response payload: ```{ \"get_weather\": { \"temperature\": 25, \"unit\": \"C\", \"condition\": \"Partly cloudy\" } }```\n",
        "<br>\n",
        "\n",
        "6. **Model → Application: Finalize the answer**\n",
        "\n",
        "* The model writes a natural-language reply (or may request another tool if needed).\n",
        "\n",
        "* Example final text: ```“It’s 25 °C, partly cloudy in Paris.”```\n",
        "<br>\n",
        "\n",
        "7. **Application → User: Deliver the response**\n",
        "\n",
        "* Your app returns the final answer (and optionally, structured data).\n",
        "\n",
        "* Example user message: ```“Paris weather: 25 °C, partly cloudy.”```\n",
        "\n",
        "<br><br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VuuJGstDRECX"
      },
      "source": [
        "## Setup (Gemini API key)\n",
        "We’ll use the Gemini API via the ```google.genai``` Python package. Grab a free API key from [Google AI Studio](https://aistudio.google.com/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AJCfa0AyY76u"
      },
      "source": [
        "## Example - Function calling flow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5CpwFR6ZLwg"
      },
      "source": [
        "### 1) Define function declarations (OpenAPI-style)\n",
        "\n",
        "* **What you define**: a function tool (set_light_values) and its JSON schema parameters.\n",
        "\n",
        "* **What the model sees**: name, description, and typed args it can propose\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CtUpjsmsY8O5"
      },
      "outputs": [],
      "source": [
        "# Define a function that the model can call to control smart lights\n",
        "set_light_values_declaration = {\n",
        "    \"name\": \"set_light_values\",\n",
        "    \"description\": \"Sets the brightness and color temperature of a light.\",\n",
        "    \"parameters\": {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "            \"brightness\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"Light level from 0 to 100. Zero is off and 100 is full brightness\",\n",
        "            },\n",
        "            \"color_temp\": {\n",
        "                \"type\": \"string\",\n",
        "                \"enum\": [\"daylight\", \"cool\", \"warm\"],\n",
        "                \"description\": \"Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\",\n",
        "            },\n",
        "        },\n",
        "        \"required\": [\"brightness\", \"color_temp\"],\n",
        "    },\n",
        "}\n",
        "\n",
        "# This is the actual function that would be called based on the model's suggestion\n",
        "def set_light_values(brightness: int, color_temp: str) -> dict[str, int | str]:\n",
        "    \"\"\"Set the brightness and color temperature of a room light. (mock API).\n",
        "\n",
        "    Args:\n",
        "        brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n",
        "        color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the set brightness and color temperature.\n",
        "    \"\"\"\n",
        "    return {\"brightness\": brightness, \"colorTemperature\": color_temp}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8WPdslsaBuU"
      },
      "source": [
        "### 2) Call the model with tool declarations\n",
        "\n",
        "**What happens**: you send prompt + tools; the model may return a function call suggestion (name + args).\n",
        "\n",
        "**Your next step**: inspect the function call proposal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbHotfZWaCAo",
        "outputId": "c3bea991-a3ec-4441-bf17-90b059b4c97c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id=None args={'brightness': 20, 'color_temp': 'warm'} name='set_light_values'\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure the client and tools\n",
        "client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "\n",
        "# ###### Uncomment to Auto-generate a FunctionDeclaration from your callable (function) ######\n",
        "# fn_decl = types.FunctionDeclaration.from_callable(callable=set_light_values, client=client)\n",
        "# tool = types.Tool(function_declarations=[fn_decl])\n",
        "# ##############\n",
        "\n",
        "tool = types.Tool(function_declarations=[set_light_values_declaration])\n",
        "config = types.GenerateContentConfig(tools=[tool])\n",
        "\n",
        "# Define user prompt\n",
        "contents = [\n",
        "    types.Content(\n",
        "        role=\"user\", parts=[types.Part(text=\"Turn the lights down to a romantic level\")]\n",
        "    )\n",
        "]\n",
        "\n",
        "# Send request with function declarations\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=contents,\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "# Response schema: https://googleapis.github.io/python-genai/genai.html#genai.types.GenerateContentResponse\n",
        "\n",
        "print(response.candidates[0].content.parts[0].function_call)\n",
        "# response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPcWqFSdauzn"
      },
      "source": [
        "### 3) Execute your function (the model doesn’t)\n",
        "\n",
        "* **What you do**: parse the name + args, run your real function, capture the result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqiUWiCjavEx",
        "outputId": "bd8b3305-547e-4ebf-d7b6-11982344b2dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Function execution result: {'brightness': 20, 'colorTemperature': 'warm'}\n"
          ]
        }
      ],
      "source": [
        "# Extract tool call details, it may not be in the first part.\n",
        "tool_call = response.candidates[0].content.parts[0].function_call\n",
        "\n",
        "if tool_call.name == \"set_light_values\":\n",
        "    result = set_light_values(**tool_call.args)\n",
        "    print(f\"Function execution result: {result}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRkk-KV3dKWc"
      },
      "source": [
        "### 4) Return the function result to the model to finalize\n",
        "\n",
        "* **Why**: the model uses the tool output to compose the final natural-language response.\n",
        "\n",
        "* **What you send**: original model content + a function response part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHOI1gEbdQ2O",
        "outputId": "62dec8e9-1fdc-4a7c-f65a-0c50cdf54914"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alright, I've set the lights to a romantic level for you!\n"
          ]
        }
      ],
      "source": [
        "# Create a function response part\n",
        "function_response_part = types.Part.from_function_response(\n",
        "    name=tool_call.name,\n",
        "    response={\"result\": result},\n",
        ")\n",
        "\n",
        "# Append function call and result of the function execution to contents\n",
        "contents.append(response.candidates[0].content) # Append the content from the model's response.\n",
        "contents.append(types.Content(role=\"user\", parts=[function_response_part])) # Append the function response\n",
        "\n",
        "final_response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    config=config,\n",
        "    contents=contents,\n",
        ")\n",
        "\n",
        "print(final_response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDHaUmMS4SHV"
      },
      "source": [
        "## Function declaration in the OpenAPI schema format\n",
        "\n",
        "* **Purpose**: Tell Gemini **what tools exist** and **how to call them**.\n",
        "\n",
        "* **Where**: You pass a ```Tool``` that contains one or more function declarations.\n",
        "\n",
        "* **Format**: Functions are defined in **JSON** using a **subset of OpenAPI/JSON Schema** for parameters.\n",
        "\n",
        "**Fields you must define**\n",
        "\n",
        "1) **name** (string) — unique, descriptive\n",
        "\n",
        "    * Use letters/nums/underscores/camelCase; no spaces.\n",
        "\n",
        "    * Example: get_weather_forecast, sendEmail.\n",
        "\n",
        "2) **description** (string) — when to use it\n",
        "\n",
        "    * Explain the purpose and capabilities; be specific.\n",
        "\n",
        "    * Example: “Finds theaters by location and optional movie title currently in theaters.”\n",
        "\n",
        "3) **parameters** (object) — input schema\n",
        "\n",
        "    * Describes the arguments the model may supply.\n",
        "\n",
        "* Inside parameters:\n",
        "\n",
        "  * **type** (string) — overall type, usually \"object\".\n",
        "\n",
        "  * **properties** (object) — each parameter is a property with its own schema:\n",
        "\n",
        "    * **type** — e.g., \"string\", \"integer\", \"boolean\", \"array\".\n",
        "\n",
        "    * **description** — what it is, format rules, examples.\n",
        "\n",
        "      * Example: “City and state, e.g., San Francisco, CA or ZIP like 95616.”\n",
        "\n",
        "    * **enum** (optional) — fixed allowed values (improves accuracy).\n",
        "\n",
        "        * Example: \"enum\": [\"daylight\",\"cool\",\"warm\"]\n",
        "\n",
        "  * **required** (array) — which parameter names are mandatory.\n",
        "\n",
        " *Complete list of supported keywords of the OpenAPI schema format [here](https://ai.google.dev/api/caching#Schema)*\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Schema fields**\n",
        "\n",
        "* type — data type (string, integer, number, boolean, array, object).\n",
        "\n",
        "* title — short human title for the schema.\n",
        "\n",
        "* description — plain-English help (Markdown ok).\n",
        "\n",
        "* nullable — whether a value can be null.\n",
        "\n",
        "* properties — fields of an object (each with its own schema).\n",
        "\n",
        "* required — property names that must be present.\n",
        "\n",
        "* minProperties / maxProperties — bounds on the number of properties in an object.\n",
        "\n",
        "* minLength / maxLength / pattern — string length and regex constraints.\n",
        "\n",
        "* enum — fixed set of allowed values (improves reliability).\n",
        "\n",
        "* format — optional refinement hint (e.g., int32).\n",
        "\n",
        "* items / minItems / maxItems — array element schema and size limits.\n",
        "\n",
        "* minimum / maximum — numeric bounds for numbers/integers.\n",
        "\n",
        "* anyOf — input is valid if it matches any one branch.\n",
        "\n",
        "* default — documentation-only default; doesn’t enforce or auto-fill.\n",
        "\n",
        "* example — example value for docs.\n",
        "\n",
        "* propertyOrdering — display/order hint (non-standard).\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "<br>\n",
        "\n",
        "**NOTE** : You can also **construct FunctionDeclarations from Python functions directly** using types.FunctionDeclaration.from_callable(client=client, callable=your_function)\n",
        "<br>\n",
        "\n",
        "**Manual VS Automated Function Declaration - Decision checklist**\n",
        "\n",
        "  * Use from_callable if:\n",
        "\n",
        "    * You need to ship a demo.\n",
        "\n",
        "    * Your inputs are simple (few fields, no complex constraints).\n",
        "\n",
        "    * You want auto-run with minimal plumbing.\n",
        "\n",
        "  * Use JSON Schema if:\n",
        "\n",
        "    * You need strict validation (enums, ranges, either/or).\n",
        "\n",
        "    * You care about consistency across teams/languages.\n",
        "\n",
        "    * You’re going production (quotas, safety, logging, stable interface).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "roM-S28qHYr4",
        "outputId": "8713791e-a127-4f3d-ddf1-252ce50988c9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FunctionDeclaration(\n",
              "  description=\"\"\"Set the brightness and color temperature of a room light. (mock API).\n",
              "\n",
              "Args:\n",
              "    brightness: Light level from 0 to 100. Zero is off and 100 is full brightness\n",
              "    color_temp: Color temperature of the light fixture, which can be `daylight`, `cool` or `warm`.\n",
              "\n",
              "Returns:\n",
              "    A dictionary containing the set brightness and color temperature.\"\"\",\n",
              "  name='set_light_values',\n",
              "  parameters=Schema(\n",
              "    properties={\n",
              "      'brightness': Schema(\n",
              "        type=<Type.INTEGER: 'INTEGER'>\n",
              "      ),\n",
              "      'color_temp': Schema(\n",
              "        type=<Type.STRING: 'STRING'>\n",
              "      )\n",
              "    },\n",
              "    required=[\n",
              "      'brightness',\n",
              "      'color_temp',\n",
              "    ],\n",
              "    type=<Type.OBJECT: 'OBJECT'>\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "types.FunctionDeclaration.from_callable(callable=set_light_values, client=client)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOz9-XWJOWxH"
      },
      "source": [
        "### Example - OpenAPI schema format\n",
        "\n",
        "* We’re giving the model a ```tool``` called get_weather so it can get current weather or a short forecast for a location (or coords).\n",
        "\n",
        "\n",
        "* In this case, we provide the flexibility to specify the location **either by its name** or **coordinates**. It can also add optional days (how many days ahead) and tags (like filters).\n",
        "\n",
        "* When a user asks, “What’s the weather in Paris?”, the model doesn’t hit an API itself. It proposes a call like get_weather({ \"location\": \"Paris, FR\", \"days\": 1 })."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aV81nXxV4bi3"
      },
      "outputs": [],
      "source": [
        "get_weather_declaration = {\n",
        "  \"name\": \"get_weather\",  # function name the model will call\n",
        "  \"description\": \"Get current weather or a short forecast for a location (or coords).\",\n",
        "  \"parameters\": {\n",
        "    \"type\": \"object\",\n",
        "    \"title\": \"Args for get_weather\",\n",
        "    \"description\": \"Inputs for get_weather.\",\n",
        "    \"nullable\": False,\n",
        "\n",
        "    \"properties\": {\n",
        "      \"location\": {\n",
        "        \"type\": \"string\",\n",
        "        \"description\": \"City/state or ZIP (e.g., 'San Francisco, CA' or '95616').\",\n",
        "        \"minLength\": 1,\n",
        "        \"maxLength\": 100,\n",
        "        \"pattern\": \".*\",\n",
        "        \"enum\": [\"San Francisco, CA\", \"Paris, FR\", \"95616\"],\n",
        "        \"nullable\": False,\n",
        "        \"default\": \"San Francisco, CA\",\n",
        "        \"example\": \"Paris, FR\"\n",
        "      },\n",
        "\n",
        "      \"days\": {\n",
        "        \"type\": \"integer\",\n",
        "        \"description\": \"Days ahead (1–7).\",\n",
        "        \"format\": \"int32\",\n",
        "        \"minimum\": 1,\n",
        "        \"maximum\": 7,\n",
        "        \"nullable\": False\n",
        "      },\n",
        "\n",
        "      \"tags\": {\n",
        "        \"type\": \"array\",\n",
        "        \"description\": \"Optional filters.\",\n",
        "        \"items\": { \"type\": \"string\" },\n",
        "        \"minItems\": 0,\n",
        "        \"maxItems\": 10,\n",
        "        \"nullable\": True\n",
        "      },\n",
        "\n",
        "      \"coords\": {\n",
        "        \"type\": \"object\",\n",
        "        \"description\": \"Geographic coordinates.\",\n",
        "        \"properties\": {\n",
        "          \"lat\": { \"type\": \"number\", \"minimum\": -90,  \"maximum\": 90  },\n",
        "          \"lon\": { \"type\": \"number\", \"minimum\": -180, \"maximum\": 180 }\n",
        "        },\n",
        "        \"required\": [\"lat\", \"lon\"]\n",
        "      }\n",
        "    },\n",
        "\n",
        "\n",
        "    \"minProperties\": 1,\n",
        "    \"maxProperties\": 10,\n",
        "\n",
        "    \"anyOf\": [\n",
        "      {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": { \"location\": { \"type\": \"string\" } },\n",
        "        \"required\": [\"location\"]\n",
        "      },\n",
        "      {\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {\n",
        "          \"coords\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "              \"lat\": { \"type\": \"number\", \"minimum\": -90,  \"maximum\": 90  },\n",
        "              \"lon\": { \"type\": \"number\", \"minimum\": -180, \"maximum\": 180 }\n",
        "            },\n",
        "            \"required\": [\"lat\", \"lon\"]\n",
        "          }\n",
        "        },\n",
        "        \"required\": [\"coords\"]\n",
        "      }\n",
        "    ],\n",
        "\n",
        "    \"propertyOrdering\": [\"location\", \"days\", \"tags\", \"coords\"]\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BbkJisMCRHEI",
        "outputId": "07e63e0b-0c69-4579-9de5-70ed06f99886"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "id=None args={'location': 'Paris, FR'} name='get_weather'\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configure the client and tools\n",
        "client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "tools = types.Tool(function_declarations=[get_weather_declaration])\n",
        "config = types.GenerateContentConfig(tools=[tools])\n",
        "\n",
        "### Scenarios ###\n",
        "# - City input path\n",
        "# Prompt: “Weather in Paris today.”\n",
        "# Matches the branch that requires location.\n",
        "\n",
        "# - Coordinates path\n",
        "# Prompt: “Forecast near 48.8566, 2.3522 for 3 days.”\n",
        "# Matches the branch that requires coords.\n",
        "\n",
        "# - Validation failure\n",
        "# Prompt: “Show forecast.” (no city/coords)\n",
        "# Your app should either ask a clarifying question or let the model ask for the missing field, based on your prompting/settings.\n",
        "#################\n",
        "\n",
        "# Define user prompt\n",
        "contents = [\n",
        "    types.Content(\n",
        "        role=\"user\", parts=[types.Part(text=\"Show forecast for paris.\")]\n",
        "    )\n",
        "]\n",
        "\n",
        "# Send request with function declarations\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=contents,\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "print(response.candidates[0].content.parts[0].function_call)\n",
        "# print(response.candidates[0].content.parts[0].text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH4phMvzO-Yx"
      },
      "source": [
        "## Automatic function calling (Python only)\n",
        "**What it is**\n",
        "\n",
        "* Pass **Python functions directly** as tools; the SDK turns them into function declarations, **executes them** when the model asks, and returns the final text to you.\n",
        "\n",
        "* Write functions with **type hints** and a **clear docstring** [Google-style recommended](https://google.github.io/styleguide/pyguide.html#383-functions-and-methods)\n",
        "<br>\n",
        "\n",
        "**What the SDK does for you**\n",
        "\n",
        "* Detects when the model wants to call a function.\n",
        "\n",
        "* Calls your Python function with the model’s args.\n",
        "\n",
        "* Sends back your function’s result to the model.\n",
        "\n",
        "* Returns the model’s final text response to your code.\n",
        "<br>\n",
        "\n",
        "**What types the SDK can describe automatically**\n",
        "* The Python SDK can auto-build a tool schema from your **type hints**.\n",
        "* It understands scalars, lists (even nested), and **Pydantic models**.\n",
        "* It does **not** do a good job with raw dict[...]\n",
        "* For strict rules (enums, ranges, either/or), write a JSON schema yourself.\n",
        "\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aE8gvoJTkOIG",
        "outputId": "c6de9030-4b86-4d60-fa6b-b96b67e767e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FunctionDeclaration(\n",
              "  description=\"Stores coordinates. coords must include keys 'lat' and 'lon'.\",\n",
              "  name='set_coords_bad',\n",
              "  parameters=Schema(\n",
              "    properties={\n",
              "      'coords': Schema(\n",
              "        type=<Type.OBJECT: 'OBJECT'>\n",
              "      )\n",
              "    },\n",
              "    required=[\n",
              "      'coords',\n",
              "    ],\n",
              "    type=<Type.OBJECT: 'OBJECT'>\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ❌ The SDK can’t derive clear, per-field structure from a dict, so you’ll typically see a very loose/ambiguous schema\n",
        "# The model won’t know which keys to supply, so calls will be vague or malformed.\n",
        "\n",
        "def set_coords_bad(coords: dict[str, float]) -> str:\n",
        "    \"\"\"Stores coordinates. coords must include keys 'lat' and 'lon'.\"\"\"\n",
        "    return \"ok\"\n",
        "\n",
        "\n",
        "bad_decl = types.FunctionDeclaration.from_callable(callable=set_coords_bad, client=client)\n",
        "bad_decl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P_or5wfzkN9U",
        "outputId": "efc12fc8-1d9a-4095-f8b8-88f19e396f33"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "FunctionDeclaration(\n",
              "  description='Stores coordinates (lat/lon).',\n",
              "  name='set_coords_good',\n",
              "  parameters=Schema(\n",
              "    properties={\n",
              "      'coords': Schema(\n",
              "        properties={\n",
              "          'lat': Schema(\n",
              "            type=<Type.NUMBER: 'NUMBER'>\n",
              "          ),\n",
              "          'lon': Schema(\n",
              "            type=<Type.NUMBER: 'NUMBER'>\n",
              "          )\n",
              "        },\n",
              "        required=[\n",
              "          'lat',\n",
              "          'lon',\n",
              "        ],\n",
              "        type=<Type.OBJECT: 'OBJECT'>\n",
              "      )\n",
              "    },\n",
              "    required=[\n",
              "      'coords',\n",
              "    ],\n",
              "    type=<Type.OBJECT: 'OBJECT'>\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ✅ Pydantic gives the SDK a named-object shape with required fields, so the model can reliably supply the right arguments.\n",
        "\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class Coords(BaseModel):\n",
        "    lat: float\n",
        "    lon: float\n",
        "\n",
        "def set_coords_good(coords: Coords) -> str:\n",
        "    \"\"\"Stores coordinates (lat/lon).\"\"\"\n",
        "    return \"ok\"\n",
        "\n",
        "good_decl = types.FunctionDeclaration.from_callable(callable=set_coords_good, client=client)\n",
        "good_decl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTWby0ZHlHGr"
      },
      "source": [
        "### Example - Automatic Function Calling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btLNrNaEPog5",
        "outputId": "0340b286-5a88-4fad-ce01-1a2f0e44bd23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The current temperature in Boston, MA is 25 degrees Celsius.\n"
          ]
        }
      ],
      "source": [
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Define the function with type hints and docstring\n",
        "def get_current_temperature(location: str) -> dict:\n",
        "    \"\"\"Gets the current temperature for a given location.\n",
        "\n",
        "    Args:\n",
        "        location: The city and state, e.g. San Francisco, CA\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the temperature and unit.\n",
        "    \"\"\"\n",
        "    # ... (implementation) ...\n",
        "    return {\"temperature\": 25, \"unit\": \"Celsius\"}\n",
        "\n",
        "# Configure the client\n",
        "client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "config = types.GenerateContentConfig(\n",
        "    tools=[get_current_temperature]\n",
        ")  # Pass the function itself\n",
        "\n",
        "# Make the request\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"What's the temperature in Boston?\",\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "print(response.text)  # The SDK handles the function call and returns the final text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M05_yVSYXavx"
      },
      "source": [
        "### **Variations you might see**\n",
        "\n",
        "* **No-tool path**: If the prompt doesn’t need external data, the model replies directly with text.\n",
        "\n",
        "* **Clarifying questio**n: If required args are missing (“What’s the weather?”), the model may ask: “Which city?”—because your schema requires location.\n",
        "\n",
        "\n",
        "* **Parallel function calling**: The model can propose multiple tool calls in one turn. Great for independent work: fetching data from multiple sources or triggering separate actions at once (e.g., CRM + Billing lookups; weather for many cities). Used when the functions are not dependent on each other.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObCxAKnriV_w",
        "outputId": "43387a24-8951-4db5-b54d-479e01a9f9e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Alright, the disco ball is spinning, the music is loud and energetic, and the lights are dimmed. The party is ready to start!\n"
          ]
        }
      ],
      "source": [
        "# PARALLEL FUNCITON CALLING EXAMPLE using automated function calling\n",
        "# Use Case: converting your apartment into a disco.\n",
        "\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "\n",
        "# Actual function implementations\n",
        "def power_disco_ball_impl(power: bool) -> dict:\n",
        "    \"\"\"Powers the spinning disco ball.\n",
        "\n",
        "    Args:\n",
        "        power: Whether to turn the disco ball on or off.\n",
        "\n",
        "    Returns:\n",
        "        A status dictionary indicating the current state.\n",
        "    \"\"\"\n",
        "    return {\"status\": f\"Disco ball powered {'on' if power else 'off'}\"}\n",
        "\n",
        "def start_music_impl(energetic: bool, loud: bool) -> dict:\n",
        "    \"\"\"Play some music matching the specified parameters.\n",
        "\n",
        "    Args:\n",
        "        energetic: Whether the music is energetic or not.\n",
        "        loud: Whether the music is loud or not.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the music settings.\n",
        "    \"\"\"\n",
        "    music_type = \"energetic\" if energetic else \"chill\"\n",
        "    volume = \"loud\" if loud else \"quiet\"\n",
        "    return {\"music_type\": music_type, \"volume\": volume}\n",
        "\n",
        "def dim_lights_impl(brightness: float) -> dict:\n",
        "    \"\"\"Dim the lights.\n",
        "\n",
        "    Args:\n",
        "        brightness: The brightness of the lights, 0.0 is off, 1.0 is full.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the new brightness setting.\n",
        "    \"\"\"\n",
        "    return {\"brightness\": brightness}\n",
        "\n",
        "# Configure the client\n",
        "client = genai.Client(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "config = types.GenerateContentConfig(\n",
        "    tools=[power_disco_ball_impl, start_music_impl, dim_lights_impl]\n",
        ")\n",
        "\n",
        "# Make the request\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    contents=\"Do everything you need to this place into party!\",\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "\n",
        "print(response.text)\n",
        "# print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxLWdtyYdNeV"
      },
      "source": [
        "* **Compositional function calling** :\n",
        "  - Compositional (sequential) function calling allows Gemini to chain multiple function calls together to fulfill a complex request.\n",
        "  - For example, to answer \"Get the temperature in my current location\", the Gemini API might first invoke a get_current_location() function followed by a get_weather() function that takes the location as a parameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLjHlxKsi9IE"
      },
      "source": [
        "## Function calling modes\n",
        "The Gemini API lets you control how the model uses the provided tools (function declarations). Specifically, you can set the mode within the ```.function_calling_config```.\n",
        "\n",
        "* **AUTO (Default)**: The model decides whether to generate a natural language response or suggest a function call based on the prompt and context. **This is the most flexible mode and recommended for most scenarios.**\n",
        "\n",
        "* **ANY**: The model is constrained to always predict a function call and guarantees function schema adherence. If ```allowed_function_names``` is not specified, the model can choose from any of the provided function declarations. If ```allowed_function_names``` is provided as a list, the model can only choose from the functions in that list. **Use this mode when you require a function call response to every prompt (if applicable)**.\n",
        "\n",
        "* **NONE**: The model is prohibited from making function calls. This is equivalent to sending a request without any function declarations. **Use this to temporarily disable function calling without removing your tool definitions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-doA4IYlmLi3"
      },
      "source": [
        "## Best practices\n",
        "* **Function and Parameter Descriptions**: Be extremely clear and specific in your descriptions. The model relies on these to choose the correct function and provide appropriate arguments.\n",
        "* **Naming**: Use descriptive, no spaces/dots/dashes.\n",
        "Good: ```getWeatherForecast / set_light_values``` • Bad: ```Get Weather! or get-weather```\n",
        "* **Strong typing**: Prefer specific types; use ```enum``` for closed sets.\n",
        "e.g., ```units: {type:\"string\", enum:[\"C\",\"F\"]}```; ```days: {type:\"integer\", minimum:1, maximum:7}```\n",
        "* **Tool selection** (keep it tight): Only include tools relevant to the turn (ideally ≤ 10–20 active)—omit unrelated tools.\n",
        "* **Prompt engineering**\n",
        "  * provide context:\n",
        "e.g., System: “You are a helpful weather assistant.”\n",
        "\n",
        "  * instruct usage: e.g., ”Use ```get_weather``` for real conditions. Don’t guess; if date missing, ask.”\n",
        "\n",
        "  * encourage clarification: e.g., “If location is missing or ambiguous, ask the user to specify city or coords.”\n",
        "* **Temperature** (determinism): Use low temp for tool calls.\n",
        "e.g., ```temperature=0``` → fewer hallucinated args.\n",
        "* **Validation** (for impactful actions): Confirm before executing.\n",
        "e.g., “About to **refund $129** for order **#A123**—should I proceed? (yes/no)”\n",
        "* **Error handling** (graceful, structured): Return actionable errors the model can summarize.\n",
        "e.g., ```{\"error\":\"inventory_timeout\",\"hint\":\"Try warehouse='DAL' or retry in 30s\"}```\n",
        "* **Token Limits**: Function descriptions and parameters count towards your input token limit - trim if hitting token limits\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
